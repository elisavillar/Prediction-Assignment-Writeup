# Set seed so the model can be reproduce
set.seed(111)
# Split dataset
train <- createDataPartition(training_df$classe, p = 0.8, list = FALSE)
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(url_train, dest = "dataset_training.csv")
training_df <- read.csv("dataset_training.csv", na.strings="?")
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url_test, dest = "dataset_testing.csv")
testing_df <- read.csv("dataset_testing.csv", na.strings="?")
dim(training_df)
dim(testing_df)
# Distribution of the variable classe
table(training_df$classe)
dim(training_df)
dim(testing_df)
# Distribution of the variable classe
table(training_df$classe)
# Load caret package
library(caret)
# Set seed so the model can be reproduce
set.seed(111)
# Split dataset
train <- createDataPartition(training_df$classe, p = 0.8, list = FALSE)
training <- train[train, ]
# Load caret package
library(caret)
# Set seed so the model can be reproduce
set.seed(111)
# Split dataset
train <- createDataPartition(training_df$classe, p = 0.8, list = FALSE)
training <- training_df[train, ]
validation <- training_df[-train, ]
control_rf <- trainControll(method="cv", 5)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
install.packages("randomForest")
install.packages("corrplot")
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(randomForest)
install.packages("randomForest")
install.packages("randomForest)
install.packages("randomForest")
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(randomForest)
install.packages("randomForest")
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
url_forest <- ("https://cran.r-project.org/src/contrib/Archive/randomForest/randomForest_4.6-14.tar.gz")
install.packages(url_forest, repos=NULL, type="source")
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(url_train, dest = "dataset_training.csv")
training_df <- read.csv("dataset_training.csv", na.strings="?")
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url_test, dest = "dataset_testing.csv")
testing_df <- read.csv("dataset_testing.csv", na.strings="?")
dim(training_df)
dim(testing_df)
# Distribution of the variable classe
table(training_df$classe)
# Set seed so the model can be reproduce
set.seed(111)
# Split dataset 60% for train and 40% for validation
train <- createDataPartition(training_df$classe, p = 0.6, list = FALSE)
training <- training_df[train, ]
validation <- training_df[-train, ]
control_rf <- trainControll(method="cv", 5)
control_rf <- trainControl(method="cv", 5)
control_rf <- trainControl(method="cv", 5)
model_rf <- train(classe ~ ., data = training, method = "rf", trControl = control_rf, ntree=250)
gc()
control_rf <- trainControl(method="cv", 5)
model_rf <- train(classe ~ ., data = training, method = "rf", trControl = control_rf, ntree=250)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(url_train, dest = "dataset_training.csv")
training_df <- read.csv("dataset_training.csv", na.strings="?")
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url_test, dest = "dataset_testing.csv")
testing_df <- read.csv("dataset_testing.csv", na.strings="?")
dim(training_df)
dim(testing_df)
# Distribution of the variable classe
table(training_df$classe)
# Set seed so the model can be reproduce
set.seed(111)
# Split dataset 60% for train and 40% for validation
train <- createDataPartition(training_df$classe, p = 0.6, list = FALSE)
training <- training_df[train, ]
validation <- training_df[-train, ]
sum(complete.cases(train_raw))
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(url_train, dest = "dataset_training.csv")
training_df <- read.csv("dataset_training.csv", na.strings="?")
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url_test, dest = "dataset_testing.csv")
testing_df <- read.csv("dataset_testing.csv", na.strings="?")
dim(training_df)
dim(testing_df)
# Distribution of the variable classe
table(training_df$classe)
sum(complete.cases(training_df))
train_raw <- training_df[, colSums(is.na(training_df)) == 0]
test_raw <- testing_df[, colSums(is.na(testing_df)) == 0]
classe <- train_raw$classe
train_remove <- grepl("^X|timestamp|window", names(train_raw))
train_raw <- train_raw[, !train_remove]
train_cleaned <- train_raw[, sapply(train_raw, is.numeric)]
train_cleaned$classe <- classe
test_remove <- grepl("^X|timestamp|window", names(test_raw))
test_raw <- test_raw[, !test_remove]
test_cleaned <- test_raw[, sapply(test_raw, is.numeric)]
# New dimension of the datasets
dim(test_cleaned)
dim(train_cleaned)
# Set seed so the model can be reproduce
set.seed(111)
# Split dataset 60% for train and 40% for validation
train <- createDataPartition(train_cleaned$classe, p = 0.6, list = FALSE)
training <- training_df[train, ]
validation <- training_df[-train, ]
control_rf <- trainControl(method="cv", 5)
model_rf <- train(classe ~ ., data = training, method = "rf", trControl = control_rf, ntree=250)
gc()
gc()
gc()
gc()
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(url_train, dest = "dataset_training.csv")
training_df <- read.csv("dataset_training.csv", na.strings="?")
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url_test, dest = "dataset_testing.csv")
testing_df <- read.csv("dataset_testing.csv", na.strings="?")
dim(training_df)
dim(testing_df)
# Distribution of the variable classe
table(training_df$classe)
sum(complete.cases(training_df))
train_raw <- training_df[, colSums(is.na(training_df)) == 0]
test_raw <- testing_df[, colSums(is.na(testing_df)) == 0]
classe <- train_raw$classe
train_remove <- grepl("^X|timestamp|window", names(train_raw))
train_raw <- train_raw[, !train_remove]
train_cleaned <- train_raw[, sapply(train_raw, is.numeric)]
train_cleaned$classe <- classe
test_remove <- grepl("^X|timestamp|window", names(test_raw))
test_raw <- test_raw[, !test_remove]
test_cleaned <- test_raw[, sapply(test_raw, is.numeric)]
# New dimension of the datasets
dim(test_cleaned)
dim(train_cleaned)
# Set seed so the model can be reproduce
set.seed(111)
# Split dataset 60% for train and 40% for validation
train <- createDataPartition(train_cleaned$classe, p = 0.6, list = FALSE)
training <- training_df[train, ]
validation <- training_df[-train, ]
control_rf <- trainControl(method="cv", 5)
model_rf <- train(classe ~ ., data = training, method = "rf", trControl = control_rf, ntree=250)
predict_rf <- predict(model_rf, validation)
memory.limit()
memory.limit(size=1800)
memory.limit(size=7000)
gc()
control_rf <- trainControl(method="cv", 5)
model_rf <- train(classe ~ ., data = training, method = "rf", trControl = control_rf, ntree=250)
# load necessary libraries
library(caret)
library(randomForest)
# read in the data and identify the NA's
traindata <- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
testdata  <- read.csv("pml-testing.csv", na.strings = c("NA", "#DIV/0!", ""))
# remove NA columns for the training and testing data
comps <- complete.cases(t(traindata)) & complete.cases(t(testdata))
traindata <- traindata[,comps]
testdata  <- testdata[,comps]
# remove columns with data that isn't useful
traindata <- traindata[,-c(1,3,4,5,6,7)]
testdata <- testdata[,-c(1,3,4,5,6,7)]
# data splitting
set.seed(111)
inTrain <- createDataPartition(traindata$classe, p=0.6, list=FALSE)
traindata2 <- traindata[inTrain,]
validation <- traindata[-inTrain,]
# fit a model
modFit <- randomForest(classe~., data=traindata2)
# the results on the training set
trainresults <- predict(modFit, traindata2)
trainacc <- sum(trainresults==traindata2$classe)/length(trainresults)
paste("Accuracy on training set =",trainacc)
## [1] "Accuracy on training set = 1"
# the results on the validation set
validresults <- predict(modFit, newdata=validation)
validacc <- sum(validresults==validation$classe)/length(validresults)
paste("Accuracy on validation set =",validacc)
## [1] "Accuracy on validation set = 0.992990058628601"
# the results on the test set
testresults <- predict(modFit, newdata=testdata)
print("Classifications on the test set:"); testresults
## [1] "Classifications on the test set:"
##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B
## Levels: A B C D E
# load necessary libraries
library(caret)
library(randomForest)
# read in the data and identify the NA's
traindata <- read.csv("dataset_training.csv", na.strings = c("NA", "#DIV/0!", ""))
testdata  <- read.csv("dataset_testing.csv", na.strings = c("NA", "#DIV/0!", ""))
# remove NA columns for the training and testing data
comps <- complete.cases(t(traindata)) & complete.cases(t(testdata))
traindata <- traindata[,comps]
testdata  <- testdata[,comps]
# remove columns with data that isn't useful
traindata <- traindata[,-c(1,3,4,5,6,7)]
testdata <- testdata[,-c(1,3,4,5,6,7)]
# data splitting
set.seed(111)
inTrain <- createDataPartition(traindata$classe, p=0.6, list=FALSE)
traindata2 <- traindata[inTrain,]
validation <- traindata[-inTrain,]
# fit a model
modFit <- randomForest(classe~., data=traindata2)
# the results on the training set
trainresults <- predict(modFit, traindata2)
trainacc <- sum(trainresults==traindata2$classe)/length(trainresults)
paste("Accuracy on training set =",trainacc)
## [1] "Accuracy on training set = 1"
# the results on the validation set
validresults <- predict(modFit, newdata=validation)
validacc <- sum(validresults==validation$classe)/length(validresults)
paste("Accuracy on validation set =",validacc)
## [1] "Accuracy on validation set = 0.992990058628601"
# the results on the test set
testresults <- predict(modFit, newdata=testdata)
print("Classifications on the test set:"); testresults
## [1] "Classifications on the test set:"
##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B
## Levels: A B C D E
# fit a model
modFit <- randomForest(classe~., data=traindata2)
ibrary(caret)
## Loading required package: lattice
## Loading required package: ggplot2
set.seed(123456)
trainset <- createDataPartition(train$classe, p = 0.8, list = FALSE)
Training <- train[trainset, ]
Validation <- train[-trainset, ]
Feature selection
First we clean up near zero variance features, columns with missing values and descriptive fields.
# exclude near zero variance features
nzvcol <- nearZeroVar(Training)
Training <- Training[, -nzvcol]
# exclude columns with m40% ore more missing values exclude descriptive
# columns like name etc
cntlength <- sapply(Training, function(x) {
sum(!(is.na(x) | x == ""))
})
nullcol <- names(cntlength[cntlength < 0.6 * length(Training$classe)])
descriptcol <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2",
"cvtd_timestamp", "new_window", "num_window")
excludecols <- c(descriptcol, nullcol)
Training <- Training[, !names(Training) %in% excludecols]
library(randomForest)
## randomForest 4.6-7
## Type rfNews() to see new features/changes/bug fixes.
rfModel <- randomForest(classe ~ ., data = Training, importance = TRUE, ntrees = 10)
ptraining <- predict(rfModel, Training)
print(confusionMatrix(ptraining, Training$classe))
## Confusion Matrix and Statistics
##
##           Reference
## Prediction    A    B    C    D    E
##          A 4464    0    0    0    0
##          B    0 3038    0    0    0
##          C    0    0 2738    0    0
##          D    0    0    0 2573    0
##          E    0    0    0    0 2886
##
## Overall Statistics
##
##                Accuracy : 1
##                  95% CI : (1, 1)
##     No Information Rate : 0.284
##     P-Value [Acc > NIR] : <2e-16
##
##                   Kappa : 1
##  Mcnemar's Test P-Value : NA
##
## Statistics by Class:
##
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             1.000    1.000    1.000    1.000    1.000
## Specificity             1.000    1.000    1.000    1.000    1.000
## Pos Pred Value          1.000    1.000    1.000    1.000    1.000
## Neg Pred Value          1.000    1.000    1.000    1.000    1.000
## Prevalence              0.284    0.194    0.174    0.164    0.184
## Detection Rate          0.284    0.194    0.174    0.164    0.184
## Detection Prevalence    0.284    0.194    0.174    0.164    0.184
## Balanced Accuracy       1.000    1.000    1.000    1.000    1.000
pvalidation <- predict(rfModel, Validation)
print(confusionMatrix(pvalidation, Validation$classe))
## Confusion Matrix and Statistics
##
##           Reference
## Prediction    A    B    C    D    E
##          A 1116    7    0    0    0
##          B    0  751    4    0    0
##          C    0    1  680    4    0
##          D    0    0    0  639    4
##          E    0    0    0    0  717
##
## Overall Statistics
##
##                Accuracy : 0.995
##                  95% CI : (0.992, 0.997)
##     No Information Rate : 0.284
##     P-Value [Acc > NIR] : <2e-16
##
##                   Kappa : 0.994
##  Mcnemar's Test P-Value : NA
##
## Statistics by Class:
##
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             1.000    0.989    0.994    0.994    0.994
## Specificity             0.998    0.999    0.998    0.999    1.000
## Pos Pred Value          0.994    0.995    0.993    0.994    1.000
## Neg Pred Value          1.000    0.997    0.999    0.999    0.999
## Prevalence              0.284    0.193    0.174    0.164    0.184
## Detection Rate          0.284    0.191    0.173    0.163    0.183
## Detection Prevalence    0.286    0.192    0.175    0.164    0.183
## Balanced Accuracy       0.999    0.994    0.996    0.996    0.997
ptest <- predict(rfModel, test)
ptest
##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B
## Levels: A B C D E
We then save the output to files according to instructions and post it to the submission page.
answers <- as.vector(ptest)
pml_write_files = function(x) {
n = length(x)
for (i in 1:n) {
filename = paste0("problem_id_", i, ".txt")
write.table(x[i], file = filename, quote = FALSE, row.names = FALSE,
col.names = FALSE)
}
}
pml_write_files(answers)
library(caret)
set.seed(123456)
trainset <- createDataPartition(train$classe, p = 0.8, list = FALSE)
downloadcsv <- function(url, nastrings) {
temp <- tempfile()
download.file(url, temp, method = "curl")
data <- read.csv(temp, na.strings = nastrings)
unlink(temp)
return(data)
}
trainurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
train <- downloadcsv(trainurl, c("", "NA", "#DIV/0!"))
testurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
test <- downloadcsv(testurl, c("", "NA", "#DIV/0!"))
library(caret)
set.seed(123456)
trainset <- createDataPartition(train$classe, p = 0.8, list = FALSE)
Training <- train[trainset, ]
Validation <- train[-trainset, ]
Feature selection
# exclude near zero variance features
nzvcol <- nearZeroVar(Training)
Training <- Training[, -nzvcol]
# exclude columns with m40% ore more missing values exclude descriptive
# columns like name etc
cntlength <- sapply(Training, function(x) {
sum(!(is.na(x) | x == ""))
})
nullcol <- names(cntlength[cntlength < 0.6 * length(Training$classe)])
descriptcol <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2",
"cvtd_timestamp", "new_window", "num_window")
excludecols <- c(descriptcol, nullcol)
Training <- Training[, !names(Training) %in% excludecols]
library(randomForest)
## randomForest 4.6-7
## Type rfNews() to see new features/changes/bug fixes.
rfModel <- randomForest(classe ~ ., data = Training, importance = TRUE, ntrees = 10)
ptraining <- predict(rfModel, Training)
print(confusionMatrix(ptraining, Training$classe))
pvalidation <- predict(rfModel, Validation)
print(confusionMatrix(pvalidation, Validation$classe))
ptest <- predict(rfModel, test)
ptest
##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B
## Levels: A B C D E
We then save the output to files according to instructions and post it to the submission page.
randomForest?
randomForest?
?randomForest
## randomForest 4.6-7
## Type rfNews() to see new features/changes/bug fixes.
rfModel <- randomForest(classe ~ ., data = Training, importance = TRUE, ntrees = 10)
library(ggplot2)
library(caret)
library(randomForest)
library(e1071)
library(gbm)
install.packages("gbm")
library(ggplot2)
library(caret)
library(randomForest)
library(e1071)
library(gbm)
library(doParallel)
install.packages("doParallel")
library(ggplot2)
library(caret)
library(randomForest)
library(e1071)
library(gbm)
library(doParallel)
library(survival)
library(splines)
library(plyr)
# load data
training <- read.csv("dataset_training.csv", na.strings=c("#DIV/0!"), row.names = 1)
testing <- read.csv("dataset_testing.csv", na.strings=c("#DIV/0!"), row.names = 1)
training <- training[, 6:dim(training)[2]]
treshold <- dim(training)[1] * 0.95
#Remove columns with more than 95% of NA or "" values
goodColumns <- !apply(training, 2, function(x) sum(is.na(x)) > treshold  || sum(x=="") > treshold)
training <- training[, goodColumns]
badColumns <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[, badColumns$nzv==FALSE]
training$classe = factor(training$classe)
#Partition rows into training and crossvalidation
inTrain <- createDataPartition(training$classe, p = 0.6)[[1]]
crossv <- training[-inTrain,]
training <- training[ inTrain,]
inTrain <- createDataPartition(crossv$classe, p = 0.75)[[1]]
crossv_test <- crossv[ -inTrain,]
crossv <- crossv[inTrain,]
testing <- testing[, 6:dim(testing)[2]]
testing <- testing[, goodColumns]
testing$classe <- NA
testing <- testing[, badColumns$nzv==FALSE]
#Train 3 different models
mod1 <- train(classe ~ ., data=training, method="rf")
library(ggplot2)
library(caret)
library(randomForest)
library(e1071)
library(gbm)
library(doParallel)
library(survival)
library(splines)
library(plyr)
# load data
training <- read.csv("dataset_training.csv", na.strings=c("#DIV/0!"), row.names = 1)
testing <- read.csv("dataset_testing.csv", na.strings=c("#DIV/0!"), row.names = 1)
training <- training[, 6:dim(training)[2]]
treshold <- dim(training)[1] * 0.95
#Remove columns with more than 95% of NA or "" values
goodColumns <- !apply(training, 2, function(x) sum(is.na(x)) > treshold  || sum(x=="") > treshold)
training <- training[, goodColumns]
badColumns <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[, badColumns$nzv==FALSE]
training$classe = factor(training$classe)
#Partition rows into training and crossvalidation
inTrain <- createDataPartition(training$classe, p = 0.6)[[1]]
crossv <- training[-inTrain,]
training <- training[ inTrain,]
inTrain <- createDataPartition(crossv$classe, p = 0.75)[[1]]
crossv_test <- crossv[ -inTrain,]
crossv <- crossv[inTrain,]
testing <- testing[, 6:dim(testing)[2]]
testing <- testing[, goodColumns]
testing$classe <- NA
testing <- testing[, badColumns$nzv==FALSE]
#Train 3 different models
mod1 <- train(classe ~ ., data=training, method="rf")
